{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732d4440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kaushiks-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Anime Recommendation System</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114519310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f865fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Indexing string columns for ALS\n",
    "indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
    "scores_df = indexer.fit(scores_df).transform(scores_df)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"anime_id\", outputCol=\"anime_idx\")\n",
    "scores_df = indexer.fit(scores_df).transform(scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338669ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 02:14:22 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:14:22 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:14:33 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:14:46 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:14:51 WARN BlockManager: Block rdd_64_4 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:51 WARN BlockManager: Block rdd_64_8 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:51 WARN BlockManager: Block rdd_64_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:52 WARN BlockManager: Block rdd_65_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:52 WARN BlockManager: Block rdd_65_4 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:52 WARN BlockManager: Block rdd_65_8 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 3.0 in stage 15.0 (TID 85)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_5 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_7 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_2 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_9 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_64_6 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 8.0 in stage 15.0 (TID 90)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2472)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2396)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2254)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1710)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:508)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:466)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4637/1598530343.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4649/388376824.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2238/1857063204.apply(Unknown Source)\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 4.0 in stage 15.0 (TID 86)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2472)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2396)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2254)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1710)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:508)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:466)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4637/1598530343.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4649/388376824.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2238/1857063204.apply(Unknown Source)\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_6 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_2 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_5 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_7 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 WARN BlockManager: Block rdd_65_9 could not be removed as it was not found on disk or in memory\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 2.0 in stage 15.0 (TID 84)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 82)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 1.0 in stage 15.0 (TID 83)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 5.0 in stage 15.0 (TID 87)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 6.0 in stage 15.0 (TID 88)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 9.0 in stage 15.0 (TID 91)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR Executor: Exception in task 7.0 in stage 15.0 (TID 89)\n",
      "java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 15.0 (TID 89),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 15.0 (TID 82),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 15.0 (TID 91),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 15.0 (TID 84),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 15.0 (TID 88),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 15.0 (TID 85),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 15.0 (TID 83),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 15.0 (TID 86),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2472)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2396)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2254)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1710)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:508)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:466)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4637/1598530343.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4649/388376824.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2238/1857063204.apply(Unknown Source)\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 15.0 (TID 87),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/03 02:14:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 15.0 (TID 90),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2124)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1698)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2472)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2396)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2254)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1710)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:508)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:466)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$4637/1598530343.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4649/388376824.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2238/1857063204.apply(Unknown Source)\n",
      "24/05/03 02:14:57 WARN TaskSetManager: Lost task 1.0 in stage 15.0 (TID 83) (kaushiks-mbp executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/05/03 02:14:57 ERROR TaskSetManager: Task 1 in stage 15.0 failed 1 times; aborting job\n",
      "24/05/03 02:14:57 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 83) (kaushiks-mbp executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1296)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:988)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:737)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/1w/q20wyxjd2p99mvr2vjwhzyl40000gn/T/ipykernel_9013/822026586.py\", line 6, in <module>\n",
      "    model = als.fit(scores_df)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train the ALS model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m als\u001b[38;5;241m.\u001b[39mfit(scores_df)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate the model by computing the RMSE on the test data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2113\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:541\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    535\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    536\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    538\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    542\u001b[0m }\n\u001b[1;32m    544\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# ALS model parameters\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_idx\", itemCol=\"anime_idx\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Train the ALS model\n",
    "model = als.fit(scores_df)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(scores_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4402297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/03 02:22:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/03 02:22:52 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:22:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , user_id, Username, anime_id, Anime Title, rating\n",
      " Schema: _c0, user_id, Username, anime_id, Anime Title, rating\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/kaushik/Desktop/Data/users-score-2023_new.csv\n",
      "24/05/03 02:23:09 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/05/03 02:23:22 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:23:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:23:28 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:23:33 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:23:45 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:23:47 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/03 02:24:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:05 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:06 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:07 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:08 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:08 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:09 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:11 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:11 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:12 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:14 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:14 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:15 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/05/03 02:24:22 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:33 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.5131798000292254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 02:24:34 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/03 02:24:58 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|user_idx|     recommendations|\n",
      "+--------+--------------------+\n",
      "|      28|[{16495, 35.53374...|\n",
      "|      31|[{16495, 43.22482...|\n",
      "|      34|[{16495, 44.04334...|\n",
      "|      53|[{16495, 65.13637...|\n",
      "|      65|[{16495, 54.89232...|\n",
      "|      78|[{16495, 35.98358...|\n",
      "|      81|[{16495, 42.61424...|\n",
      "|      85|[{16495, 42.57969...|\n",
      "|     101|[{16495, 44.97214...|\n",
      "|     108|[{16495, 45.18070...|\n",
      "|     115|[{16495, 39.83249...|\n",
      "|     126|[{16495, 39.19075...|\n",
      "|     133|[{16495, 44.8857}...|\n",
      "|     137|[{16495, 48.79183...|\n",
      "|     148|[{16495, 52.17040...|\n",
      "|     155|[{16495, 40.66792...|\n",
      "|     183|[{16495, 44.80581...|\n",
      "|     193|[{16495, 42.88467...|\n",
      "|     210|[{16495, 45.71884...|\n",
      "|     211|[{16495, 50.24815...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, user_id: int, Username: string, anime_id: int, Anime Title: string, rating: int, user_idx: double, anime_idx: double]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session with increased memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Anime Recommendation System\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Data paths\n",
    "anime_data_path = \"/Users/kaushik/Desktop/Data/anime-dataset-2023_new.csv\"\n",
    "user_details_path = \"/Users/kaushik/Desktop/Data/users-details-2023_new.csv\"\n",
    "user_scores_path = \"/Users/kaushik/Desktop/Data/users-score-2023_new.csv\"\n",
    "\n",
    "# Read the datasets\n",
    "anime_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(anime_data_path)\n",
    "users_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(user_details_path)\n",
    "scores_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(user_scores_path)\n",
    "\n",
    "# Indexing string columns for ALS\n",
    "indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
    "scores_df = indexer.fit(scores_df).transform(scores_df)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"anime_id\", outputCol=\"anime_idx\")\n",
    "scores_df = indexer.fit(scores_df).transform(scores_df)\n",
    "\n",
    "# Repartition scores_df to manage memory better and avoid data skew\n",
    "scores_df = scores_df.repartition(200)\n",
    "\n",
    "# Cache the DataFrame to optimize read performance\n",
    "scores_df.persist()\n",
    "\n",
    "# ALS model parameters\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_idx\", itemCol=\"anime_idx\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\", nonnegative=True, blockSize=4096)\n",
    "\n",
    "# Train the ALS model\n",
    "model = als.fit(scores_df)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(scores_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 anime recommendations for each user\n",
    "user_recs = model.recommendForAllUsers(10)\n",
    "user_recs.show()\n",
    "\n",
    "# Clean up cached data\n",
    "scores_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0471a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/03 02:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/03 02:54:05 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:05 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:14 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:18 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:22 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:23 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:25 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/03 02:54:26 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:28 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:29 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:30 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:31 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:32 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:33 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:34 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:35 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:36 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:38 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:54:41 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "24/05/03 02:55:09 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------+----------------+\n",
      "|user_idx|Name                                                        |predicted_rating|\n",
      "+--------+------------------------------------------------------------+----------------+\n",
      "|28      |Haha Xiong Juchang: Zhihui Da Jiangtang                     |52.258476       |\n",
      "|28      |Amrita no Kyouen                                            |41.644794       |\n",
      "|28      |Heya Camp: Sauna to Gohan to Sanrin Bike                   |37.71732        |\n",
      "|28      |Buggy Map                                                   |34.61948        |\n",
      "|28      |Choujigen Game Neptune: HiLight                            |30.94444        |\n",
      "|28      |YuuYuuHakusho: Eizou Hakusho - Opening Ending Encyclopedia|27.65014        |\n",
      "|28      |Ling Jian Zun 3rd Season                                    |27.508638       |\n",
      "|28      |Zen-chan Two-chan                                           |27.48463        |\n",
      "|28      |Oshiri Tantei 4th Season                                    |27.186192       |\n",
      "|28      |Wan Jie Chun Qiu                                            |27.038176       |\n",
      "|31      |Haha Xiong Juchang: Zhihui Da Jiangtang                     |44.85151        |\n",
      "|31      |Amrita no Kyouen                                            |37.37577        |\n",
      "|31      |Buggy Map                                                   |28.724934       |\n",
      "|31      |YuuYuuHakusho: Eizou Hakusho - Opening Ending Encyclopedia|22.07093        |\n",
      "|31      |Xin Datou Er Zi He Xiao Tou Baba: Chuanqi Doudou            |21.976696       |\n",
      "|31      |Choujigen Game Neptune: HiLight                            |21.229492       |\n",
      "|31      |Blue Hawaii feat. Crush Penomeco                            |20.262596       |\n",
      "|31      |Oshiri Tantei 4th Season                                    |20.257954       |\n",
      "|31      |Washimo 8th Season                                          |18.63344        |\n",
      "|31      |Ginga Eiyuu Densetsu: Die Neue These - Gekitotsu            |18.572529       |\n",
      "+--------+------------------------------------------------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 02:55:10 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:55:11 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:55:11 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "24/05/03 02:55:20 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "24/05/03 02:55:27 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "24/05/03 02:55:33 WARN DAGScheduler: Broadcasting large task binary with size 8.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.7251361645877417\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Anime Recommendation System\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Data paths\n",
    "anime_data_path = \"/Users/kaushik/Desktop/Data/anime-dataset-2023_new.csv\"\n",
    "user_details_path = \"/Users/kaushik/Desktop/Data/users-details-2023_new.csv\"\n",
    "user_scores_path = \"/Users/kaushik/Desktop/Data/users-score-2023_new.csv\"\n",
    "\n",
    "# Read the datasets\n",
    "anime_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(anime_data_path)\n",
    "users_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(user_details_path)\n",
    "scores_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"escape\", '\"').load(user_scores_path)\n",
    "\n",
    "# Indexing the anime and user IDs in scores_df\n",
    "anime_indexer = StringIndexer(inputCol=\"anime_id\", outputCol=\"anime_idx\", handleInvalid=\"skip\")\n",
    "scores_df = anime_indexer.fit(scores_df).transform(scores_df)\n",
    "\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\", handleInvalid=\"skip\")\n",
    "scores_df = user_indexer.fit(scores_df).transform(scores_df)\n",
    "\n",
    "# Indexing the anime IDs in anime_df for joining later\n",
    "anime_df = anime_indexer.fit(anime_df).transform(anime_df)\n",
    "\n",
    "# ALS model parameters\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_idx\", itemCol=\"anime_idx\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\", nonnegative=True)\n",
    "\n",
    "# Train the ALS model\n",
    "model = als.fit(scores_df)\n",
    "\n",
    "# Generate recommendations\n",
    "user_recs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Explode the recommendations into separate rows\n",
    "expanded_recs = user_recs.withColumn(\"recommendations\", explode(\"recommendations\")).select(\n",
    "    col(\"user_idx\"),\n",
    "    col(\"recommendations.anime_idx\").alias(\"anime_idx\"),\n",
    "    col(\"recommendations.rating\").alias(\"predicted_rating\")\n",
    ")\n",
    "\n",
    "# Join with the anime DataFrame to get the titles\n",
    "final_recommendations = expanded_recs.join(anime_df, \"anime_idx\").select(\n",
    "    \"user_idx\", \"Name\", \"predicted_rating\"\n",
    ")\n",
    "\n",
    "# Show the final recommendations\n",
    "final_recommendations.show(truncate=False)\n",
    "\n",
    "# Evaluate the model by computing the RMSE\n",
    "predictions = model.transform(scores_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04182ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
